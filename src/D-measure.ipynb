{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import euclidean\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from scipy.stats import entropy\n",
    "import nibabel as nib\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fc_matrix(file_path):\n",
    "    \"\"\" Load functional connectivity matrix from a .pconn.nii file. \"\"\"\n",
    "    img = nib.load(file_path)\n",
    "    fc_matrix = img.get_fdata()\n",
    "    return fc_matrix\n",
    "\n",
    "def create_knn_graph(fc_matrix, k=5):\n",
    "    \"\"\" Create a graph from a functional connectivity matrix using k-nearest neighbors based on absolute values. \"\"\"\n",
    "    n = fc_matrix.shape[0]  # Number of nodes\n",
    "    G = nx.Graph()\n",
    "    for i in range(n):\n",
    "        G.add_node(i)\n",
    "    \n",
    "    # For each node, add edges to the k-nearest neighbors based on absolute values of connectivity strengths\n",
    "    for i in range(n):\n",
    "        # Sort indices based on the absolute values, get the k highest values indices for each row\n",
    "        indices = np.argsort(np.abs(fc_matrix[i]))[-k:]\n",
    "        for j in indices:\n",
    "            if i != j:  # Ensure no self-loops\n",
    "                G.add_edge(i, j, weight=fc_matrix[i][j])\n",
    "    \n",
    "    return G\n",
    "def create_threshold_graph(fc_matrix, std_multiplier=2):\n",
    "    \"\"\"\n",
    "    Create a graph from a functional connectivity matrix by adding edges where the \n",
    "    absolute connection strength is above a threshold defined as a multiple of the\n",
    "    standard deviation of the absolute values in the connectivity matrix.\n",
    "    \"\"\"\n",
    "    n = fc_matrix.shape[0]  # Number of nodes\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Calculate the threshold as std_multiplier times the standard deviation of the absolute values\n",
    "    threshold = std_multiplier * np.std(np.abs(fc_matrix))\n",
    "    \n",
    "    # Add nodes\n",
    "    for i in range(n):\n",
    "        G.add_node(i)\n",
    "    \n",
    "    # Add edges based on the threshold\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i != j and np.abs(fc_matrix[i, j]) > threshold:  # Avoid self-loops and check threshold\n",
    "                G.add_edge(i, j, weight=fc_matrix[i, j])\n",
    "    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D-measure (distance) between G1 and G2: 999997.7169783808\n",
      "D-measure (distance) between G1 and G2: 0.0\n"
     ]
    }
   ],
   "source": [
    "def compute_features(graph):\n",
    "    features = {}\n",
    "    \n",
    "    # Degree distribution\n",
    "    degrees = [d for n, d in graph.degree()]\n",
    "    features['degree_mean'] = np.mean(degrees)\n",
    "    features['degree_std'] = np.std(degrees)\n",
    "    \n",
    "    # Clustering coefficient\n",
    "    clustering_coeffs = list(nx.clustering(graph).values())\n",
    "    features['clustering_mean'] = np.mean(clustering_coeffs)\n",
    "    features['clustering_std'] = np.std(clustering_coeffs)\n",
    "    \n",
    "    # Average shortest path length\n",
    "    if nx.is_connected(graph):\n",
    "        features['avg_shortest_path'] = nx.average_shortest_path_length(graph)\n",
    "    else:\n",
    "        features['avg_shortest_path'] = np.nan  # Use NaN for disconnected graph\n",
    "    \n",
    "    return features\n",
    "\n",
    "def compute_feature_vector(graph):\n",
    "    features = compute_features(graph)\n",
    "    feature_vector = np.array([features['degree_mean'], features['degree_std'], \n",
    "                               features['clustering_mean'], features['clustering_std'], \n",
    "                               features['avg_shortest_path']])\n",
    "    # Handle NaN values by replacing them with a large finite value\n",
    "    feature_vector = np.nan_to_num(feature_vector, nan=1e6)\n",
    "    return feature_vector\n",
    "\n",
    "def compute_graph_distance(fv1, fv2):\n",
    "    return euclidean(fv1, fv2)\n",
    "\n",
    "# Example graphs\n",
    "G1 = nx.erdos_renyi_graph(100, 0.05)\n",
    "G2 = nx.erdos_renyi_graph(100, 0.1)\n",
    "\n",
    "# Compute feature vectors\n",
    "fv1 = compute_feature_vector(G1)\n",
    "fv2 = compute_feature_vector(G2)\n",
    "\n",
    "# Compute distance\n",
    "distance = compute_graph_distance(fv1, fv2)\n",
    "print(f'D-measure (distance) between G1 and G2: {distance}')\n",
    "# Compute distance\n",
    "distance = compute_graph_distance(fv2, fv2)\n",
    "print(f'D-measure (distance) between G1 and G2: {distance}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D-measure (distance) between G1 and G2: 2.556388957922372\n",
      "D-measure (distance) between G2 and G2: 0.0\n"
     ]
    }
   ],
   "source": [
    "def compute_nnd(graph):\n",
    "    if not nx.is_connected(graph):\n",
    "        largest_cc = max(nx.connected_components(graph), key=len)\n",
    "        graph = graph.subgraph(largest_cc).copy()\n",
    "    \n",
    "    N = len(graph.nodes)\n",
    "    if N == 1:\n",
    "        return 0\n",
    "    \n",
    "    d = nx.diameter(graph)\n",
    "    distance_distribution = np.zeros(d + 1)\n",
    "    \n",
    "    for node in graph.nodes:\n",
    "        lengths = nx.single_source_shortest_path_length(graph, node)\n",
    "        for length in lengths.values():\n",
    "            if length <= d:\n",
    "                distance_distribution[length] += 1\n",
    "    \n",
    "    distance_distribution /= (N * (N - 1))\n",
    "    distance_distribution = np.clip(distance_distribution, 1e-10, None)  # Avoid log(0)\n",
    "    nnd = np.sum(distance_distribution * np.log(1 / distance_distribution)) / np.log(d + 1)\n",
    "    \n",
    "    return nnd\n",
    "\n",
    "def degree_distribution(graph):\n",
    "    degrees = [degree for node, degree in graph.degree()]\n",
    "    hist, bins = np.histogram(degrees, bins=range(1, max(degrees) + 2), density=True)\n",
    "    return hist\n",
    "\n",
    "def pad_distributions(dist1, dist2):\n",
    "    max_length = max(len(dist1), len(dist2))\n",
    "    dist1 = np.pad(dist1, (0, max_length - len(dist1)), 'constant')\n",
    "    dist2 = np.pad(dist2, (0, max_length - len(dist2)), 'constant')\n",
    "    return dist1, dist2\n",
    "\n",
    "def graph_dissimilarity(G, G_prime, w1=1, w2=1, w3=1):\n",
    "    # Average node distance difference\n",
    "    mu_G = np.mean([len(nx.single_source_shortest_path_length(G, source=i)) - 1 for i in G.nodes])\n",
    "    mu_G_prime = np.mean([len(nx.single_source_shortest_path_length(G_prime, source=i)) - 1 for i in G_prime.nodes])\n",
    "    mu_diff = abs(mu_G - mu_G_prime)\n",
    "    \n",
    "    # NND difference\n",
    "    nnd_G = compute_nnd(G)\n",
    "    nnd_G_prime = compute_nnd(G_prime)\n",
    "    nnd_diff = abs(nnd_G - nnd_G_prime)\n",
    "    \n",
    "    # Jensen-Shannon divergence for node distance distributions\n",
    "    degree_dist_G = degree_distribution(G)\n",
    "    degree_dist_G_prime = degree_distribution(G_prime)\n",
    "    degree_dist_G, degree_dist_G_prime = pad_distributions(degree_dist_G, degree_dist_G_prime)\n",
    "    js_divergence = jensenshannon(degree_dist_G, degree_dist_G_prime)\n",
    "    \n",
    "    # Jensen-Shannon divergence for complement graphs\n",
    "    G_c = nx.complement(G)\n",
    "    G_prime_c = nx.complement(G_prime)\n",
    "    degree_dist_G_c = degree_distribution(G_c)\n",
    "    degree_dist_G_prime_c = degree_distribution(G_prime_c)\n",
    "    degree_dist_G_c, degree_dist_G_prime_c = pad_distributions(degree_dist_G_c, degree_dist_G_prime_c)\n",
    "    js_divergence_complement = jensenshannon(degree_dist_G_c, degree_dist_G_prime_c)\n",
    "    \n",
    "    dissimilarity = (w1 * mu_diff + w2 * nnd_diff + \n",
    "                     (w3 / 2) * js_divergence + (w3 / 2) * js_divergence_complement)\n",
    "    \n",
    "    return dissimilarity\n",
    "\n",
    "# Compute D-measure dissimilarity\n",
    "dissimilarity = graph_dissimilarity(G1, G2)\n",
    "print(f'D-measure (distance) between G1 and G2: {dissimilarity}')\n",
    "\n",
    "# Compute D-measure dissimilarity between G2 and itself\n",
    "dissimilarity_self = graph_dissimilarity(G2, G2)\n",
    "print(f'D-measure (distance) between G2 and G2: {dissimilarity_self}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_197176/3779735107.py:15: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  bags = behavior_source.groupby('Group').apply(lambda x: x.sample(n=3)).reset_index(drop=True)\n",
      "/tmp/ipykernel_197176/3779735107.py:21: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  remaining_samples = remaining_behavior_source.groupby('Group').apply(lambda x: x.sample(n=1)).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "# Set the behavior path and list all files\n",
    "behavior_path = '/home/tico/Desktop/master_classes/project/behavior/'\n",
    "behavior_files = os.listdir(behavior_path)\n",
    "\n",
    "# Read the first file and initialize the dataframe\n",
    "behavior_source = pd.read_csv(behavior_path + behavior_files[0], sep='\\t')\n",
    "for behavior_file in behavior_files[1:]:\n",
    "    curr_behavior_source = pd.read_csv(behavior_path + behavior_file, sep='\\t')\n",
    "    behavior_source = pd.concat([behavior_source, curr_behavior_source], axis=0)\n",
    "\n",
    "# Select the relevant columns\n",
    "behavior_source = behavior_source[[\"session_id\", \"Group\"]]\n",
    "\n",
    "# Group by 'Group' and sample 3 from each group\n",
    "bags = behavior_source.groupby('Group').apply(lambda x: x.sample(n=3)).reset_index(drop=True)\n",
    "\n",
    "# Filter out the samples in 'bags' from 'behavior_source' using 'session_id'\n",
    "remaining_behavior_source = behavior_source[~behavior_source['session_id'].isin(bags['session_id'])]\n",
    "\n",
    "# Group by 'Group' and sample 1 from each group\n",
    "remaining_samples = remaining_behavior_source.groupby('Group').apply(lambda x: x.sample(n=1)).reset_index(drop=True)\n",
    "\n",
    "# Combine session_ids from bags and remaining_samples to form pconn_files\n",
    "session_ids = pd.concat([bags['session_id'], remaining_samples['session_id']])\n",
    "pconn_files = [f\"{session_id}.pconn.nii\" for session_id in session_ids]\n",
    "\n",
    "# Directory setup\n",
    "directory = \"/home/tico/Desktop/master_classes/project/BSNIP/pconn\"\n",
    "\n",
    "# Load graphs\n",
    "graphs = {}\n",
    "for file_name in pconn_files:\n",
    "    file_path = os.path.join(directory, file_name)\n",
    "    fc_matrix = load_fc_matrix(file_path)\n",
    "    graphs[file_name] = create_threshold_graph(fc_matrix, std_multiplier=2)\n",
    "\n",
    "# Compute similarities\n",
    "similarity_matrix = pd.DataFrame(index=session_ids, columns=session_ids)\n",
    "for session_id1 in session_ids:\n",
    "    for session_id2 in session_ids:\n",
    "        print(f'processing: {session_id1}, {session_id2}')\n",
    "        if session_id1 != session_id2:\n",
    "            graph1 = graphs[f\"{session_id1}.pconn.nii\"]\n",
    "            graph2 = graphs[f\"{session_id2}.pconn.nii\"]\n",
    "            similarity_matrix.loc[session_id1, session_id2] = graph_dissimilarity(graph1, graph2)\n",
    "        else:\n",
    "            similarity_matrix.loc[session_id1, session_id2] = 0  # Self similarity is zero\n",
    "\n",
    "# Sort the matrix\n",
    "sorted_similarity_matrix = similarity_matrix.sort_index(axis=0).sort_index(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(sorted_similarity_matrix, cmap=\"YlGnBu\", annot=True, fmt=\".2f\")\n",
    "plt.title(\"Graph Similarity Matrix\")\n",
    "plt.xlabel(\"Sessions\")\n",
    "plt.ylabel(\"Sessions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
