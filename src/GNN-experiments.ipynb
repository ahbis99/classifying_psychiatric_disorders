{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "import nibabel as nib\n",
    "import networkx as nx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing .pconn.nii files:   2%|‚ñè         | 12/638 [00:05<05:16,  1.98it/s]"
     ]
    }
   ],
   "source": [
    "# Directory containing the pconn files\n",
    "directory = \"/home/tico/Desktop/master_classes/project/BSNIP/pconn\"\n",
    "pconn_files = [f for f in os.listdir(directory) if f.endswith('.pconn.nii')]\n",
    "std = 2\n",
    "behavior_path = '/home/tico/Desktop/master_classes/project/behavior/'\n",
    "behavior_files = os.listdir(behavior_path)\n",
    "\n",
    "# Load behavior data\n",
    "behavior_source = pd.read_csv(os.path.join(behavior_path, behavior_files[0]), sep='\\t')\n",
    "for behavior_file in behavior_files[1:]:\n",
    "    curr_behavior_source = pd.read_csv(os.path.join(behavior_path, behavior_file), sep='\\t')\n",
    "    behavior_source = pd.concat([behavior_source, curr_behavior_source], axis=0)\n",
    "behavior_source = behavior_source[[\"session_id\", \"Group\"]]\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "behavior_source['Group'] = label_encoder.fit_transform(behavior_source['Group'])\n",
    "\n",
    "def load_fc_matrix(file_path):\n",
    "    \"\"\" Load functional connectivity matrix from a .pconn.nii file. \"\"\"\n",
    "    img = nib.load(file_path)\n",
    "    fc_matrix = img.get_fdata()\n",
    "    return fc_matrix\n",
    "\n",
    "def create_threshold_graph(fc_matrix, std_multiplier=2):\n",
    "    \"\"\"\n",
    "    Create a graph from a functional connectivity matrix by adding edges where the \n",
    "    absolute connection strength is above a threshold defined as a multiple of the\n",
    "    standard deviation of the absolute values in the connectivity matrix.\n",
    "    \"\"\"\n",
    "    n = fc_matrix.shape[0]  # Number of nodes\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Calculate the threshold as std_multiplier times the standard deviation of the absolute values\n",
    "    threshold = std_multiplier * np.std(np.abs(fc_matrix))\n",
    "    \n",
    "    # Add nodes\n",
    "    for i in range(n):\n",
    "        G.add_node(i, feature=torch.tensor(fc_matrix[i], dtype=torch.float))\n",
    "    \n",
    "    # Add edges based on the threshold\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):  # Avoid self-loops and duplicate edges\n",
    "            if np.abs(fc_matrix[i, j]) > threshold:\n",
    "                G.add_edge(i, j, weight=fc_matrix[i, j])\n",
    "    \n",
    "    # Extract edge_index and node features from the graph\n",
    "    edge_index = torch.tensor(list(G.edges)).t().contiguous()\n",
    "    x = torch.stack([G.nodes[i]['feature'] for i in range(n)], dim=0)\n",
    "    \n",
    "    return edge_index, x\n",
    "\n",
    "# Prepare a list to store the results\n",
    "data_list = []\n",
    "\n",
    "for file_name in tqdm(pconn_files, desc=\"Processing .pconn.nii files\"):\n",
    "    fc_file_path = os.path.join(directory, file_name)\n",
    "    session_id = file_name[:-len('.pconn.nii')]\n",
    "    label = behavior_source.loc[behavior_source['session_id'] == session_id, 'Group'].values[0]\n",
    "    fc_matrix = load_fc_matrix(fc_file_path)\n",
    "    edge_index, x = create_threshold_graph(fc_matrix, std)\n",
    "    data = Data(x=x, edge_index=edge_index, y=torch.tensor([label], dtype=torch.long))\n",
    "    data_list.append(data)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_data, test_data = train_test_split(data_list, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create DataLoader for training and testing\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GNN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.fc1 = torch.nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.fc1(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Check input dimension\n",
    "input_dim = data_list[0].x.size(1)\n",
    "\n",
    "# Initialize the GNN model, optimizer, and loss function\n",
    "model = GNN(input_dim=input_dim, hidden_dim=64, output_dim=len(label_encoder.classes_))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Training function\n",
    "def train(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    for data in loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        loss = criterion(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Testing function\n",
    "def test(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += (pred == data.y).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(20):\n",
    "    train(model, train_loader, optimizer, criterion)\n",
    "    train_acc = test(model, train_loader)\n",
    "    test_acc = test(model, test_loader)\n",
    "    print(f'Epoch {epoch+1}, Train Accuracy: {train_acc:.4f}, Test Accuracy: {test_acc:.4f}')\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_accuracy = test(model, test_loader)\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
